{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sección 1: Estudio Detallado de K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "2uk7SFS2jCfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ¿Qué es K-Nearest Neighbors (KNN)?\n",
        "    \n",
        "  K-Nearest Neighbors (KNN) o K-Vecinos más Cercanos es uno de los algoritmos más simples e intuitivos de **Aprendizaje Supervisado** (Supervised Learning). Se le conoce como un algoritmo \\\"basado en instancias\\\" o \\\"perezoso\\\" (*lazy algorithm*).\n",
        "    \n",
        "  **¿Por qué \\\"perezoso\\\"?:** A diferencia de otros algoritmos (como la Regresión Logística o las SVM) que construyen un modelo explícito durante la fase de \\\"entrenamiento\\\", KNN no \\\"aprende\\\" nada. Simplemente **memoriza todo el conjunto de datos de entrenamiento**.\n",
        "  \n",
        "  El verdadero \\\"trabajo\\\" ocurre durante la fase de **predicción** o **inferencia**.\n",
        "\n",
        "  KNN puede usarse tanto para **Clasificación** como para **Regresión**.\n",
        "  \n",
        "  * **Clasificación:** Asigna la clase más común entre sus K vecinos más cercanos.\n",
        "  * **Regresión:** Predice el valor promedio de los K vecinos más cercanos."
      ],
      "metadata": {
        "id": "iD1gIHJljGeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ¿Cómo Funciona el Algoritmo? (Clasificación)\n",
        "\n",
        "  Imaginemos que tenemos un nuevo punto de datos (un punto \\\"incógnita\\\") al que queremos asignarle una clase.\n",
        "    \n",
        "  1.  **Elegir un valor para K:** K es el número de \\\"vecinos\\\" que el algoritmo considerará. Es un hiperparámetro clave. (Ej. K=5).\n",
        "  2.  **Calcular Distancias:** Calcula la distancia entre el punto incógnita y **todos** los puntos del conjunto de entrenamiento. La métrica de distancia más común es la **Distancia Euclidiana**.\n",
        "  3.  **Encontrar los K Vecinos:** Identifica los K puntos de entrenamiento que tienen las distancias más cortas al punto incógnita.\n",
        "  4.  **Votar por la Clase:** Mira las etiquetas (clases) de esos K vecinos. La clase que más se repite (la \\\"moda\\\") se asigna como la predicción para el punto incógnita."
      ],
      "metadata": {
        "id": "xHIIsXZHk28I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. El Hiperparámetro Clave: K\n",
        "\n",
        "  La elección de K es crucial y afecta directamente el **balance entre sesgo y varianza** (*bias-variance tradeoff*).\n",
        "\n",
        "  * **K pequeño (ej. K=1):**\n",
        "      * **Bajo Sesgo (Low Bias):** El modelo es muy flexible y se adapta mucho a los datos de entrenamiento.\n",
        "      * **Alta Varianza (High Variance):** Es muy sensible al ruido y a los *outliers*. Un solo punto atípico puede cambiar una predicción. Es propenso al **sobreajuste (Overfitting)**.\n",
        "  \n",
        "  * **K grande (ej. K=N, donde N es total de puntos):**\n",
        "       * **Alto Sesgo (High Bias):** El modelo es muy simple (siempre predecirá la clase mayoritaria de todo el dataset).\n",
        "       * **Baja Varianza (Low Variance):** Es muy estable, pero pierde los patrones locales. Es propenso al **subajuste (Underfitting)**.\n",
        "\n",
        "  **¿Cómo elegir K?**\n",
        "  Generalmente se usa **Validación Cruzada (Cross-Validation)**. Se prueba un rango de valores de K (usualmente impares, para evitar empates) y se elige el que dé la mejor métrica de evaluación (ej. accuracy, F1-score) en el conjunto de validación. Un método común es el \\\"Método del Codo\\\" (Elbow Method)."
      ],
      "metadata": {
        "id": "bRpwUJ9qmpS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Métricas de Distancia\n",
        "    \n",
        "  KNN depende fundamentalmente de la \"distancia\". La elección de la métrica puede cambiar los resultados.\n",
        "  \n",
        "  Dados dos puntos $p$ y $q$ en un espacio de $n$ dimensiones ($p = (p_1, p_2, ..., p_n)$ y $q = (q_1, q_2, ..., q_n)$):\n",
        "    \n",
        "  ### a) Distancia Euclidiana ($L_2$)\n",
        "  \"Es la distancia \\\"en línea recta\\\" que todos conocemos. Es la más usada.\n",
        "    \"$$ d(p, q) = \\\\sqrt{\\\\sum_{i=1}^{n} (p_i - q_i)^2} $$\n",
        "  \n",
        "  ### b) Distancia de Manhattan ($L_1$)\n",
        "  Es la suma de las distancias absolutas de las coordenadas. Útil en espacios de alta dimensionalidad o cuando las \\\"calles\\\" (features) son ortogonales.\\n\",\n",
        "    \"$$ d(p, q) = \\\\sum_{i=1}^{n} |p_i - q_i| $$\n",
        "  \n",
        "  ### c) Distancia de Minkowski ($L_p$)\n",
        "  Es una generalización de las dos anteriores. El parámetro $p$ define la métrica:\n",
        "    \"$$ d(p, q) = \\\\left( \\\\sum_{i=1}^{n} |p_i - q_i|^p \\\\right)^{1/p} $$\\n\",\n",
        "  \n",
        "  * Si $p=1$, es la Distancia de Manhattan.\n",
        "  * Si $p=2$, es la Distancia Euclidiana."
      ],
      "metadata": {
        "id": "HAacI9s5nUoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ¡IMPORTANTE! La Necesidad de Escalar los Datos\n",
        "    \n",
        "  Este es uno de los puntos más críticos de KNN. **El algoritmo es extremadamente sensible a la escala de las características (features).**\n",
        "    \n",
        "  Imagina que tienes un dataset para predecir si un cliente comprará, con dos features:\n",
        "  1.  `edad` (rango: 18 - 80)\n",
        "  2.  `ingreso_anual` (rango: 20,000 - 150,000)\n",
        "    \n",
        "  Si usamos la distancia euclidiana, la diferencia en `ingreso_anual` (que está en decenas de miles) **dominará completamente** el cálculo de la distancia, haciendo que la `edad` sea prácticamente irrelevante.\n",
        "    \n",
        "  **Solución:** Siempre se deben escalar los datos antes de usar KNN. Métodos comunes:\n",
        "  * **Estandarización (StandardScaler):** Transforma los datos para que tengan media 0 y desviación estándar 1. (Recomendado si hay outliers).\n",
        "  * **Normalización (MinMaxScaler):** Escala los datos a un rango fijo, comúnmente [0, 1]."
      ],
      "metadata": {
        "id": "1PVNJsJGtC_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Implementación (Código)"
      ],
      "metadata": {
        "id": "3TsXHgzqtxdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7U4liKYXT2S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Clases de Scikit-learn\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\\n\",\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Implementación de KNN \"Desde Cero\""
      ],
      "metadata": {
        "id": "vTBTCK76uRo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular la distancia euclidiana\n",
        "def euclidean_distance(x1, x2):\n",
        "      return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "class KNN_from_scratch:\n",
        "  def __init__(self, k=3):\n",
        "      self.k = k\n",
        "\n",
        "\n",
        "  def fit(self, X, y):\\n\",\n",
        "     # KNN es \\\"perezoso\\\", solo memoriza los datos\n",
        "     self.X_train = X\n",
        "     self.y_train = y\n",
        "\n",
        "  def predict(self, X_test):\n",
        "      # Hacemos predicciones para cada punto en X_test\n",
        "      y_pred = [self._predict_point(x) for x in X_test]\n",
        "      return np.array(y_pred)\n",
        "\n",
        "  def _predict_point(self, x):\n",
        "     # 1. Calcular distancias a todos los puntos de entrenamiento\n",
        "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "    # 2. Obtener los índices de los K vecinos más cercanos\n",
        "    # np.argsort() devuelve los índices que ordenarían el array\n",
        "    k_nearest_indices = np.argsort(distances)[:self.k]\n",
        "\n",
        "    # 3. Obtener las etiquetas (clases) de esos K vecinos\n",
        "    k_nearest_labels = [self.y_train[i] for i in k_nearest_indices]\n",
        "\n",
        "    # 4. Votar: Devolver la clase más común (la moda)\n",
        "    # Counter(...).most_common(1) devuelve [(etiqueta, conteo)]\n",
        "    most_common = Counter(k_nearest_labels).most_common(1)\n",
        "    return most_common[0][0]"
      ],
      "metadata": {
        "id": "OjA4_8-WuUnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. Implementación con `scikit-learn` (Uso Real)\n",
        "\n",
        "Ahora usemos la implementación optimizada de `scikit-learn` con el famoso dataset Iris."
      ],
      "metadata": {
        "id": "pwsnC5M2wkaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar Datos\\n\",\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Dividir en Entrenamiento y Prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Escalar los Datos (¡Muy importante!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Encontrar el mejor K (Método del Codo)\n",
        "k_range = range(1, 21)\n",
        "accuracy_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    # Usamos validación cruzada (5-fold) en los datos de entrenamiento\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    accuracy_scores.append(scores.mean())\n",
        "\n",
        "    # Graficar los resultados\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_range, accuracy_scores, marker='o', linestyle='dashed')\n",
        "    plt.title('Precisión vs. Valor de K (Método del Codo)')\n",
        "    plt.xlabel('Valor de K')\n",
        "    plt.ylabel('Precisión (Validación Cruzada)')\n",
        "    plt.xticks(k_range)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Elegir el mejor K (el que maximiza la precisión)\n",
        "    best_k = k_range[np.argmax(accuracy_scores)]\n",
        "    print(f\\\"El mejor valor de K encontrado es:\" {best_k})"
      ],
      "metadata": {
        "id": "vy0pc-o_wpjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Entrenar y Evaluar el Modelo Final con el mejor K"
      ],
      "metadata": {
        "id": "8gsmH550x_3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(f\\\"Entrenando modelo final con K={best_k}...\")\n",
        "\n",
        "    # Instanciamos el clasificador de Scikit-learn\\n\",\n",
        "    knn_final = KNeighborsClassifier(n_neighbors=best_k)\n",
        "\n",
        "    # \\\"Entrenamos\\\" (memorizamos) los datos escalados\n",
        "    knn_final.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Hacemos predicciones en el conjunto de prueba\n",
        "    y_pred = knn_final.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "av35ZaPoyI7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluar el rendimiento"
      ],
      "metadata": {
        "id": "rCpnV58eycZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\\\"\\\\nPrecisión (Accuracy) en el conjunto de prueba:\" {accuracy:.4f})\n",
        "\n",
        "    print(\\\"\\\\nReporte de Clasificación:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "    print(\\\"\\\\nMatriz de Confusión:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-8I0IIMdyfSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Ventajas y Desventajas de KNN\n",
        "    \n",
        "  ### Ventajas\n",
        "  1.  **Simple e Intuitivo:** Fácil de entender y explicar.\n",
        "  2.  **Sin Fase de Entrenamiento:** Como es \\\"perezoso\\\", la \\\"capacitación\\\" es instantánea (solo almacena datos).\n",
        "  3.  **Flexible:** Se adapta fácilmente a nuevos datos de entrenamiento (solo hay que agregarlos al dataset memorizado).\n",
        "  4.  **No Paramétrico:** No hace suposiciones sobre la distribución de los datos (ej. no asume que son linealmente separables).\n",
        "  5.  **Bueno para Tareas No Lineales:** Puede capturar fronteras de decisión complejas.\n",
        "  \n",
        "  ### Desventajas\n",
        "  1.  **Costo Computacional en Predicción:** ¡Es muy lento para predecir! Debe calcular la distancia a *todos* los puntos de entrenamiento por cada nueva predicción. (Complejidad $O(N*D)$ donde N=muestras, D=features).\n",
        "\n",
        "  2.  **La Maldición de la Dimensionalidad (*Curse of Dimensionality*):**\n",
        "      * En espacios con muchas dimensiones (muchas features), todos los puntos tienden a estar \\\"lejos\\\" entre sí.\n",
        "      * El concepto de \\\"vecino cercano\\\" pierde sentido.\n",
        "      * El rendimiento se degrada rápidamente con $D$ alto.\n",
        "      \n",
        "  3.  **Sensible a Features Irrelevantes:** Si hay muchas features que no aportan información (ruido), estas pueden opacar a las features útiles en el cálculo de la distancia.\n",
        "  4.  **Requiere Escalamiento de Datos:** Como vimos, es obligatorio.\n",
        "  5.  **Necesita Muchos Datos:** Requiere una buena densidad de datos para que el concepto de \"vecino\" sea significativo. Consume mucha memoria (almacena todo $X_{train}$)."
      ],
      "metadata": {
        "id": "XtHDkhJgyyPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sección 2: Aplicación Hipotética y Modificación del Algoritmo\n",
        "    \n",
        "  Basándonos en el estudio anterior, vamos a plantear un escenario donde el KNN estándar falla y cómo podemos adaptarlo.\n"
      ],
      "metadata": {
        "id": "4xR84kJezYK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. El Problema: Datos Desbalanceados\n",
        "    \n",
        "  **Aplicación Hipotética:** Detección de Fraude en Transacciones.\n",
        "    \n",
        "  Tenemos un dataset de transacciones bancarias. La gran mayoría (ej. 99.5%) son legítimas (Clase 0) y una pequeñísima minoría (0.5%) son fraudulentas (Clase 1).\n",
        "    \n",
        "  **¿Por qué falla el KNN estándar (voto por mayoría)?**\n",
        "    \n",
        "  Imagina que usamos K=10 para clasificar una nueva transacción (punto $X_{nuevo}$).\n",
        "  Debido a que la Clase 0 (legítima) es abrumadoramente dominante, es **extremadamente probable** que la mayoría (quizás 8, 9 o incluso los 10) de los vecinos más cercanos pertenezcan a la Clase 0, simplemente porque hay muchos más.\n",
        "  \n",
        "  Puede que el vecino *más cercano* (el #1) sea de Clase 1 (fraude), pero si los otros 9 son de Clase 0, el voto por mayoría predecirá Clase 0. El modelo estará fuertemente sesgado hacia la clase mayoritaria.\n",
        "  \n",
        "## 2. La Modificación: KNN Ponderado (Weighted KNN)\n",
        "\n",
        "  **Idea Central:** No todos los vecinos deben tener el mismo peso en la votación. **Un vecino que está más cerca debe tener más influencia que un vecino que está más lejos.**\n",
        "  \n",
        "  **La Adaptación (Cómo funciona):**\n",
        "\n",
        "  1.  Encuentra los K vecinos más cercanos y sus distancias (igual que antes).\n",
        "  2.  Calcula un **peso (weight)** para cada vecino. Una función común es la inversa de la distancia:\n",
        "      $$ weight_i = \\\\frac{1}{distance_i + \\\\epsilon} $$\n",
        "      (Se añade un épsilon, un número muy pequeño, para evitar la división por cero si la distancia es 0).\n",
        "  3.  **Votación Ponderada:** En lugar de que cada vecino dé 1 voto, cada vecino da un voto igual a su `peso`.\n",
        "      * Se suman los pesos de todos los vecinos que pertenecen a la Clase 0.\n",
        "      * Se suman los pesos de todos los vecinos que pertenecen a la Clase 1.\n",
        "  4.  **Predicción:** La clase que acumule el mayor peso total gana la votación.\n",
        "    \n",
        "  **¿Por qué esto soluciona el problema?**\n",
        "  Volviendo al ejemplo: Si el vecino #1 (Clase 1) está a una distancia de 0.1, y los otros 9 vecinos (Clase 0) están a distancias de 0.8, 0.9, 1.0, etc...\n",
        "\n",
        "  El peso del vecino de Clase 1 (ej. $1/0.1 = 10$) será mucho mayor que la suma de los pesos de los vecinos de Clase 0 (ej. $1/0.8 + 1/0.9 + ... < 10$).\n",
        "    \n",
        "  El **KNN Ponderado** le da más importancia a la *proximidad* que al simple *conteo*."
      ],
      "metadata": {
        "id": "NxWU6v8vzpS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementación de KNN Ponderado\n",
        "    \n",
        "  ### 3.1. Modificando nuestra clase \"Desde Cero\""
      ],
      "metadata": {
        "id": "mp9z32tG0iza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Weighted_KNN_from_scratch:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.epsilon = 1e-6 # Para evitar división por cero\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_pred = [self._predict_point(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict_point(self, x):\n",
        "        # 1. Calcular distancias\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # 2. Obtener índices y distancias de los K vecinos\n",
        "        k_nearest_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_distances = [distances[i] for i in k_nearest_indices]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_nearest_indices]\n",
        "\n",
        "        # 3. Calcular Pesos (¡Aquí está la modificación!)\n",
        "        weights = [1 / (d + self.epsilon) for d in k_nearest_distances]\n",
        "\n",
        "        # 4. Votación Ponderada\n",
        "        class_weights = {}\n",
        "        for i in range(self.k):\n",
        "            label = k_nearest_labels[i]\n",
        "            weight = weights[i]\n",
        "\n",
        "            if label not in class_weights:\n",
        "                class_weights[label] = 0\n",
        "            class_weights[label] += weight\n",
        "\n",
        "        # 5. Devolver la clase con el mayor peso acumulado\n",
        "        # (max(..., key=...) devuelve la clave del diccionario con el valor máximo)\n",
        "        return max(class_weights, key=class_weights.get)"
      ],
      "metadata": {
        "id": "-ZuV9zSa0tjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Comparación en `scikit-learn`\n",
        "    \n",
        "  Afortunadamente, `scikit-learn` ya tiene esta funcionalidad implementada con el hiperparámetro `weights`.\n",
        "    \n",
        "  * `weights='uniform'` (Default): Voto por mayoría estándar. Todos los vecinos pesan 1.\n",
        "  * `weights='distance'` (Modificado): Voto ponderado por la inversa de la distancia (justo lo que implementamos).\n",
        "  \n",
        "  Vamos a crear un dataset sintético **desbalanceado** y comparar el rendimiento."
      ],
      "metadata": {
        "id": "rASzBJPg17tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Crear un dataset con 2 features, 1000 muestras, 95% de Clase 0 y 5% de Clase 1\n",
        "X_imb, y_imb = make_classification(n_samples=1000,\n",
        "                                 n_features=2,\n",
        "                                 n_informative=2,\n",
        "                                 n_redundant=0,\n",
        "                                 n_clusters_per_class=1,\n",
        "                                 weights=[0.95, 0.05], # <-- ¡Desbalanceado!,\n",
        "                                 flip_y=0,\n",
        "                                 random_state=42)\n",
        "\n",
        "print(f\\\"Forma de X: \" {X_imb.shape})\n",
        "print(f\\\"Conteo de clases: \" {Counter(y_imb)})\n",
        "\n",
        "# Graficar los datos desbalanceados\n",
        "sns.scatterplot(x=X_imb[:, 0], y=X_imb[:, 1], hue=y_imb, style=y_imb, palette={0: 'blue', 1: 'red'})\n",
        "plt.title('Dataset Sintético Desbalanceado (0=Azul, 1=Rojo)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "IYJk3Cal2Hn5",
        "outputId": "f39d1efc-ae07-4a1e-ce4e-402b8dcf6753"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unexpected character after line continuation character (ipython-input-1648855124.py, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1648855124.py\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    print(f\\\"Forma de X: {X_imb.shape})\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir y escalar los datos desbalanceados\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb)\n",
        "\n",
        "scaler_imb = StandardScaler()\n",
        "X_train_imb_scaled = scaler_imb.fit_transform(X_train_imb)\n",
        "X_test_imb_scaled = scaler_imb.transform(X_test_imb)\n",
        "\n",
        "# K fijo para la comparación (ej. K=7)\n",
        "K_COMPARE = 7\n",
        "\n",
        "# Modelo 1: KNN Estándar (weights='uniform')\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=K_COMPARE, weights='uniform')\n",
        "knn_uniform.fit(X_train_imb_scaled, y_train_imb)\n",
        "y_pred_uniform = knn_uniform.predict(X_test_imb_scaled)\n",
        "\n",
        "# Modelo 2: KNN Ponderado (weights='distance')\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=K_COMPARE, weights='distance')\n",
        "knn_distance.fit(X_train_imb_scaled, y_train_imb)\n",
        "y_pred_distance = knn_distance.predict(X_test_imb_scaled)\n",
        "\n",
        "# --- Comparar Resultados ---\n",
        "# En datos desbalanceados, 'accuracy' es engañoso. Usamos 'classification_report' (F1-score, Recall)\n",
        "\n",
        "print(\\\"--- RESULTADOS KNN ESTÁNDAR (weights='uniform') ---\\\")\n",
        "print(classification_report(y_test_imb, y_pred_uniform, target_names=['Clase 0 (Legítima)', 'Clase 1 (Fraude)']_)\n",
        "\n",
        "print(\\\"\\\\n--- RESULTADOS KNN PONDERADO ---\\ \" (weights='distance'))\n",
        "print(classification_report(y_test_imb, y_pred_distance, target_names=['Clase 0 (Legítima)', 'Clase 1 (Fraude)']))"
      ],
      "metadata": {
        "id": "NaXHU0RX2h8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Análisis de la Comparación\n",
        "    \n",
        "  Al ejecutar el código anterior, deberías notar lo siguiente (los números exactos pueden variar):\n",
        "    \n",
        "  1.  **KNN Estándar (`uniform`):**\n",
        "      * Tendrá un `recall` (Sensibilidad) muy bajo para la Clase 1 (Fraude). Esto significa que falla en identificar la mayoría de los casos de fraude. Simplemente predice Clase 0 casi siempre.\n",
        "    \n",
        "  2.  **KNN Ponderado (`distance`):**\n",
        "      * Tendrá un `recall` para la Clase 1 significativamente mejor.\n",
        "      * Al dar más peso a los vecinos *muy* cercanos, es capaz de identificar correctamente instancias de la clase minoritaria (fraude) aunque esté rodeada por puntos de la clase mayoritaria.\n",
        "    \n",
        "  **Conclusión de la Adaptación:**\n",
        "  El estudio detallado de la Sección 1 nos mostró que KNN se basa en \"distancia\" y \"votación\". La aplicación hipotética (datos desbalanceados) reveló una debilidad en la \"votación\" estándar (voto por mayoría).\n",
        "    \n",
        "  La modificación (KNN Ponderado) cambia el mecanismo de \"votación\" para incorporar la \"distancia\", creando un modelo mucho más robusto para este caso de uso específico."
      ],
      "metadata": {
        "id": "lKMJGGJn3lhu"
      }
    }
  ]
}